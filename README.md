# âš–ï¸ Code du Travail Tunisien - RAG System with Multi-Stage Reasoning

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)](https://streamlit.io/)

Un systÃ¨me RAG (Retrieval-Augmented Generation) intelligent pour le Code du Travail Tunisien, utilisant un raisonnement multi-Ã©tapes et une mÃ©moire conversationnelle pour fournir des rÃ©ponses juridiques prÃ©cises et contextuelles.

## ğŸŒŸ FonctionnalitÃ©s Principales

### ğŸ§  Raisonnement Multi-Ã‰tapes (3 Stages)
1. **Reformulation de la question** : Transforme la question utilisateur en requÃªtes de recherche optimales
2. **Analyse juridique approfondie** : Analyse les articles du Code du Travail dans leur contexte
3. **RÃ©ponse humaine et actionnable** : GÃ©nÃ¨re une rÃ©ponse claire avec conseils pratiques

### ğŸ’­ MÃ©moire Conversationnelle
- **MÃ©moire court-terme** : Maintient le contexte de la session active
- **MÃ©moire long-terme** : Sauvegarde l'historique dans Qdrant pour rÃ©fÃ©rence future
- **Recherche contextuelle** : RÃ©cupÃ¨re les conversations pertinentes passÃ©es

### ğŸ” Retrieval AvancÃ©
- Recherche vectorielle avec Qdrant Cloud
- Multi-query retrieval avec dÃ©duplication
- Re-ranking basÃ© sur la pertinence
- Support des filtres hiÃ©rarchiques (Livre, Titre, Chapitre, Section, Article)

### ğŸ¨ Interface Utilisateur Moderne
- Interface chat intuitive avec Streamlit
- Affichage des sources juridiques avec scores de pertinence
- Visualisation de la chaÃ®ne de rÃ©flexion (optionnel)
- Statistiques en temps rÃ©el

## ğŸ“‹ Table des MatiÃ¨res

- [Architecture](#-architecture)
- [Installation](#-installation)
- [Configuration](#ï¸-configuration)
- [Utilisation](#-utilisation)
- [Structure du Projet](#-structure-du-projet)
- [API Documentation](#-api-documentation)
- [Exemples](#-exemples)
- [Technologies](#-technologies)
- [Contribuer](#-contribuer)
- [License](#-license)

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit UI   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI API   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
    â†“         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Memory  â”‚ â”‚   Reasoning  â”‚
â”‚ System  â”‚ â”‚    Engine    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚             â”‚
     â”‚         â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”
     â”‚         â”‚        â”‚
     â†“         â†“        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚ Qdrant  â”‚ â”‚Groq  â”‚ â”‚Searchâ”‚
â”‚  Cloud  â”‚ â”‚ LLM  â”‚ â”‚Vectorâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜
```

### Pipeline de Traitement

```
User Question
     â”‚
     â†“
[Memory Context Retrieval]
     â”‚
     â†“
[Stage 1: Query Rewriting] â†’ Optimized Queries
     â”‚
     â†“
[Vector Search in Qdrant] â†’ Relevant Articles
     â”‚
     â†“
[Stage 2: Legal Analysis] â†’ Deep Analysis
     â”‚
     â†“
[Stage 3: Human Response] â†’ Final Answer
     â”‚
     â†“
[Save to Memory]
     â”‚
     â†“
Response to User
```

## ğŸš€ Installation

### PrÃ©requis

- Python 3.8+
- Compte Qdrant Cloud (gratuit)
- ClÃ© API Groq (gratuit)

### Ã‰tapes d'Installation

1. **Cloner le repository**
```bash
git clone https://github.com/yourusername/code-travail-rag.git
cd code-travail-rag
```

2. **CrÃ©er un environnement virtuel**
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows
```

3. **Installer les dÃ©pendances**
```bash
pip install -r requirements.txt
```

4. **Configuration des clÃ©s API**

CrÃ©ez un fichier `.env` Ã  la racine :
```env
GROQ_API_KEY=your_groq_api_key_here
QDRANT_URL=your_qdrant_cloud_url
QDRANT_API_KEY=your_qdrant_api_key
```

Ou modifiez directement dans `api.py` et les modules concernÃ©s.

5. **PrÃ©parer les donnÃ©es**

```bash
# Chunking du PDF
python src/modules/chunking.py

# Embedding et upload vers Qdrant
python src/modules/embedding.py
```

## âš™ï¸ Configuration

### Structure du Projet

```
code-travail-rag/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ modules/
â”‚       â”œâ”€â”€ chunking.py          # Extraction et dÃ©coupage du PDF
â”‚       â”œâ”€â”€ embedding.py         # GÃ©nÃ©ration des embeddings
â”‚       â”œâ”€â”€ retrieval.py         # SystÃ¨me de recherche
â”‚       â”œâ”€â”€ reasoning.py         # Moteur de raisonnement 3 Ã©tapes
â”‚       â”œâ”€â”€ memory.py            # MÃ©moire conversationnelle
â”‚       â””â”€â”€ ingestion.py         # Ingestion de documents
â”œâ”€â”€ api.py                       # API FastAPI
â”œâ”€â”€ app.py                       # Interface Streamlit
â”œâ”€â”€ data/
â”‚   â””â”€â”€ TN_Code_du_Travail.pdf  # PDF source
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â””â”€â”€ README.md
```

### Configuration des Modules

#### Chunking
```python
@dataclass
class ChunkingConfig:
    # 1 article = 1 chunk
    # Les sous-articles (5-2, 5-3) sont des chunks sÃ©parÃ©s
```

#### Embedding
```python
@dataclass
class EmbeddingConfig:
    model_name: str = "sentence-transformers/all-MiniLM-L6-v2"
    vector_size: int = 384
    batch_size: int = 100
```

#### Reasoning
```python
@dataclass
class ThinkingConfig:
    model_name: str = "llama-3.3-70b-versatile"
    temperature_query_rewrite: float = 0.1
    temperature_reasoning: float = 0.2
    temperature_response: float = 0.3
```

#### Memory
```python
@dataclass
class MemoryConfig:
    short_term_limit: int = 10
    long_term_retrieval_limit: int = 3
    relevance_threshold: float = 0.6
```

## ğŸ“– Utilisation

### 1. DÃ©marrer l'API

```bash
uvicorn api:app --reload --host 0.0.0.0 --port 8000
```

API disponible sur : `http://localhost:8000`
Documentation interactive : `http://localhost:8000/docs`

### 2. Lancer l'Interface Streamlit

```bash
streamlit run app.py
```

Interface disponible sur : `http://localhost:8501`


## ğŸ› ï¸ Technologies

### Backend
- **FastAPI** : Framework API moderne et performant
- **Groq** : Inference LLM ultra-rapide (Llama 3.3 70B)
- **Qdrant Cloud** : Base de donnÃ©es vectorielle
- **Sentence Transformers** : GÃ©nÃ©ration d'embeddings

### Frontend
- **Streamlit** : Interface utilisateur interactive

### Traitement
- **PyPDF2** / **pdfplumber** : Extraction de PDF
- **LangDetect** : DÃ©tection de langue
- **Python-docx** : Support DOCX

### Models
- **all-MiniLM-L6-v2** : Embeddings (384 dimensions)
- **Llama 3.3 70B** : GÃ©nÃ©ration de rÃ©ponses



</div>
